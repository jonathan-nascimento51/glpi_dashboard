# GLPI Dashboard Fluentd Configuration
# ====================================

<system>
  log_level info
  suppress_repeated_stacktrace true
  emit_error_log_interval 30s
  suppress_config_dump
  without_source
</system>

# Input sources
# =============

# Docker container logs
<source>
  @type forward
  @id input_forward
  @label @mainstream
  port 24224
  bind 0.0.0.0
</source>

# Nginx access logs
<source>
  @type tail
  @id input_nginx_access
  @label @mainstream
  path /var/log/nginx/access.log
  pos_file /var/log/fluentd-positions/nginx-access.log.pos
  tag nginx.access
  <parse>
    @type nginx
    format /^(?<remote>[^ ]*) (?<host>[^ ]*) (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^\"]*?)(?: +\S*)?)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")?$/
    time_format %d/%b/%Y:%H:%M:%S %z
  </parse>
</source>

# Nginx error logs
<source>
  @type tail
  @id input_nginx_error
  @label @mainstream
  path /var/log/nginx/error.log
  pos_file /var/log/fluentd-positions/nginx-error.log.pos
  tag nginx.error
  <parse>
    @type multiline
    format_firstline /^\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}/
    format1 /^(?<time>\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}) \[(?<log_level>\w+)\] (?<pid>\d+).(?<tid>\d+): (?<message>.*)/
  </parse>
</source>

# Backend application logs
<source>
  @type tail
  @id input_backend_logs
  @label @mainstream
  path /var/log/backend/*.log
  pos_file /var/log/fluentd-positions/backend.log.pos
  tag backend.app
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%L%z
  </parse>
</source>

# Redis logs
<source>
  @type tail
  @id input_redis_logs
  @label @mainstream
  path /var/log/redis/redis-server.log
  pos_file /var/log/fluentd-positions/redis.log.pos
  tag redis.server
  <parse>
    @type regexp
    expression /^(?<pid>\d+):(?<role>\w+) (?<time>\d{2} \w{3} \d{4} \d{2}:\d{2}:\d{2}.\d{3}) (?<level>\w+) (?<message>.*)/
    time_format %d %b %Y %H:%M:%S.%L
  </parse>
</source>

# Main processing pipeline
# =======================

<label @mainstream>
  # Add common fields
  <filter **>
    @type record_transformer
    @id filter_add_common_fields
    <record>
      hostname "#{Socket.gethostname}"
      environment "#{ENV['ENVIRONMENT'] || 'development'}"
      service_name "#{ENV['SERVICE_NAME'] || 'glpi-dashboard'}"
      timestamp ${time}
    </record>
  </filter>

  # Parse and enrich nginx logs
  <filter nginx.**>
    @type record_transformer
    @id filter_nginx_enrichment
    <record>
      service nginx
      log_type ${tag_parts[1]}
    </record>
  </filter>

  # Parse and enrich backend logs
  <filter backend.**>
    @type record_transformer
    @id filter_backend_enrichment
    <record>
      service backend
      log_type application
    </record>
  </filter>

  # Parse and enrich redis logs
  <filter redis.**>
    @type record_transformer
    @id filter_redis_enrichment
    <record>
      service redis
      log_type server
    </record>
  </filter>

  # Filter out health check requests
  <filter nginx.access>
    @type grep
    @id filter_exclude_health_checks
    <exclude>
      key path
      pattern ^/(health|status|ping)$
    </exclude>
  </filter>

  # Add GeoIP information for nginx access logs
  <filter nginx.access>
    @type geoip
    @id filter_geoip
    geoip_lookup_keys remote
    <record>
      geoip_country_name ${country.names.en["remote"]}
      geoip_country_code ${country.iso_code["remote"]}
      geoip_city_name ${city.names.en["remote"]}
      geoip_latitude ${location.latitude["remote"]}
      geoip_longitude ${location.longitude["remote"]}
    </record>
    skip_adding_null_record true
  </filter>

  # Rate limiting for high-volume logs
  <filter **>
    @type sampling
    @id filter_sampling
    sampling_rate 100
    sample_unit tag
  </filter>

  # Output configurations
  # ====================

  # Send to Elasticsearch (if available)
  <match **>
    @type elasticsearch
    @id output_elasticsearch
    host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
    port "#{ENV['ELASTICSEARCH_PORT'] || 9200}"
    scheme "#{ENV['ELASTICSEARCH_SCHEME'] || 'http'}"
    user "#{ENV['ELASTICSEARCH_USER']}"
    password "#{ENV['ELASTICSEARCH_PASSWORD']}"
    index_name glpi-dashboard-logs
    type_name _doc
    include_timestamp true
    reconnect_on_error true
    reload_on_failure true
    reload_connections false
    request_timeout 5s
    <buffer>
      @type file
      path /var/log/fluentd-buffers/elasticsearch
      flush_mode interval
      flush_interval 10s
      flush_at_shutdown true
      retry_type exponential_backoff
      retry_wait 1s
      retry_max_interval 60s
      retry_timeout 60m
      queued_chunks_limit_size 1024
      compress gzip
    </buffer>
  </match>

  # Fallback to file output
  <match **>
    @type file
    @id output_file
    path /var/log/fluentd-output/glpi-dashboard
    append true
    time_slice_format %Y%m%d%H
    time_slice_wait 1m
    time_format %Y-%m-%dT%H:%M:%S.%L%z
    <buffer time>
      @type file
      path /var/log/fluentd-buffers/file
      flush_mode interval
      flush_interval 30s
      flush_at_shutdown true
      timekey 3600
      timekey_wait 1m
      compress gzip
    </buffer>
    <format>
      @type json
    </format>
  </match>
</label>

# Error handling
# ==============

<label @ERROR>
  <match **>
    @type file
    @id output_error
    path /var/log/fluentd-output/error
    append true
    <format>
      @type json
    </format>
  </match>
</label>